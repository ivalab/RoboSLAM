<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Online Binary Feature Learning for Loop Closure</title>
<meta name="description" content="Research efforts related to closed-loop SLAM.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="robots" content="all,follow">
<meta name="googlebot" content="index,follow,snippet,archive">
<link rel="stylesheet" href="https://ivalab.github.io/RoboSLAM/public/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="https://ivalab.github.io/RoboSLAM/public/css/font-awesome.min.css">
<link rel="stylesheet" href="https://ivalab.github.io/RoboSLAM/public/css/owl.carousel.css">
<link rel="stylesheet" href="https://ivalab.github.io/RoboSLAM/public/css/owl.theme.css">


  <link href="https://ivalab.github.io/RoboSLAM/public/css/style.ivalab.css" rel="stylesheet" id="theme-stylesheet">

 

  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  


<link href="https://ivalab.github.io/RoboSLAM/public/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="https://ivalab.github.io/RoboSLAM/public/img/favicon.png">


</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">

  <center style="margin-top:0px;margin-bottom:0px">
  <A HREF="https://ivalab.gatech.edu">
  <IMG SRC="https://ivalab.gatech.edu/ivalab.png">
  </A>
  </center>
    <a href="https://ivalab.github.io/RoboSLAM/public/">
    <h1 style="text-align:center;font-variant:small-caps" class="sidebar-heading">
    RoboSLAM Research
    </h1>
    </a>
    
      <p class="sidebar-p">Research efforts related to simultaneous localization and mapping algorithms for real-time, closed-loop operation of autonomous robots.</p>
    
    <ul class="sidebar-menu">
      
        <li><a href="https://ivalab.github.io/RoboSLAM/public/about/">About</a></li>
      
        <li><a href="https://ivalab.github.io/RoboSLAM/public/robots/">Robots</a></li>
      
        <li><a href="https://ivalab.github.io/RoboSLAM/public/research/">Research</a></li>
      
    </ul>

    <p class="social">
  
  
  
  
  
  
  
  
  <a href="https://github.com/ivalab" data-animate-hover="pulse" class="external">
    <i class="fa fa-github"></i>
  </a>
  
  
  <a href="https://github.com/ivaROS" data-animate-hover="pulse" class="external">
    <i class="fa fa-github"></i>
  </a>
  
  
  <a href="https://www.youtube.com/channel/UCKQ7FxpjoK-1l3Bp2qXCiYw/" data-animate-hover="pulse" class="external">
    <i class="fa fa-youtube"></i>
  </a>
  
  
</p>


  Select by Tags:<BR>
  
    <a href="https://ivalab.github.io/RoboSLAM/public/tags/benchmark/"
    style="padding:0px 5px 0px 5px">Benchmark</a>
  
    <a href="https://ivalab.github.io/RoboSLAM/public/tags/goodfeats/"
    style="padding:0px 5px 0px 5px">GoodFeats</a>
  
    <a href="https://ivalab.github.io/RoboSLAM/public/tags/loopclose/"
    style="padding:0px 5px 0px 5px">LoopClose</a>
  
    <a href="https://ivalab.github.io/RoboSLAM/public/tags/map2frame/"
    style="padding:0px 5px 0px 5px">Map2Frame</a>
  
  <a href="https://ivalab.github.io/RoboSLAM/public/" style="padding:0px 5px 0px 5px">All</a>

    <div class="support" style="padding-top:15px">
      <p class="credit">
        
          Research supported in part by National Science Foundation.  Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. 
        
      </p>
      <p class="support">
        <p class="credit">
      
        
          
            <a target="_blank" href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1816138&amp;HistoricalAwards=false">NSF &#35;1816138 </a><BR>
          
        
      
        
          
            AFRL
          
        
      
        </p>
      </p>
    </div>
    <div class="copyright">
      <p class="credit">
        
        Template by <a href="https://bootstrapious.com/free-templates" class="external">Bootstrapious.com</a>

&amp; ported to Hugo by <a href="https://github.com/kishaningithub">Kishan B</a>

      </p>
    </div>
  </div>

</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left"> <i class="fa fa-align-left"> </i>Menu</button>
  <h1 class="small-navbar-heading"><a href="https://ivalab.github.io/RoboSLAM/public/">RoboSLAM Research</a></h1>
</div>

  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>Online Binary Feature Learning for Loop Closure</h1>
         <p><strong>Abstract:</strong>
Loop closure is an essential module in long-term SLAM, as detecting and recognizing
re-visited locations serves to bound localization drift and correct for
geometric map distortions. It also provides a mechanism to recover from
track loss. However, most methods employ offline learnt maps that cannot
be customized to the feature space of new environments. This work explores
a means to generate a loop closing map with online adaptive properties.</p>
<h4 id="motivation-and-background">Motivation and Background</h4>
<p>Loop closure is what I would call the <em>the lost robot problem</em> (in the vein
of the <em>kidnapped robot problem</em>).  No matter how much one wishes for
a SLAM system to succeed, there are times when it just might fail.
The local map process, if there is one, usually prevent track loss by
maintaining a sense of location. This sense of location is achieved by
recognizing and exploiting high-frequency or low time interval
re-visits; imagine going to another room then back right away.  Keeping
a short term <em>memory</em> of recent observations to generate the data
re-association will improve localization.
However, if the re-visit spans a long enough distance or time, such that
the <em>memory</em> component cannot succeed, then more global data
re-association methods are needed; imagine going from home to
work/school/grandma&rsquo;s/etc.  Welcome to <em>loop closure</em>.</p>
<p>Loop closure is a tricky problem to solve. Make a mistake and it is over
for your map; unless you have a really good way of detecting mistakes
during map re-optimization.
Miss a re-visit and you stay lost; that&rsquo;s certainly happened to me lots
of times while driving!
Robots just don&rsquo;t have the corrective strategies humans have, so we
roboticists have to balance the trade-off between detection and error
rate.  In our language, that&rsquo;s called a precision-recall trade-off.
As someone whose background is control and dynamical systems and whose
lab likes to think in these terms, we thought it would benefit the
loop-closure process if there were a way to incorporate motion
insensitivity into the detection process (full credit goes to Guangcong
for the initial idea during brainstorming sessions).
Mathematically, the ideal outcome would be to have motion invariance, but
theoeretically proving and then deriving an algorithm for this may or may not
provide dividends per my colleague Stefano Soatto&rsquo;s findings.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>
Sometimes the answer leads to an efficient algorithm, sometimes the
computational complexity is too high.  More importantly, insensitivity
is often much easier to implement in practice and has nice computational
or numerical properties arising from the fact that insensitivity is like
an <em>approximate invariance</em> (my words).  In controls, we love
insensitivity to nuisance signals. Unlike invariance, insensitivity can
be dialed up or down. Furthermore, there is usually a computational
relationship with insensitivity level, which means that both can be
modulated together to identify the best operating point across these two
factors.  That idea fundamentally motivates and describes our SLAM
research.</p>
<h4 id="learning-descriptors-for-loop-closure">Learning Descriptors for Loop-Closure</h4>
<p>The loop-closure solution builds from ideas found in the IBuiLD <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>
loop closure implementation, as well as from empirically-adaptive
descriptors like BOLD <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.  Whereas BOLD aims for affine invariance of
the binary descriptor by masking out bit coordinates sensitive to affine
transformations, we mask out the ones that are sensitive across frames
(i.e., temporal motion insensitivity).
While BOLD is backed up by empirical evidence for its design, we focused
on establishing core properties of the masked descriptors in order to
confirm that the necessary topological properties of the masked Hamming
space supported the data association needs of loop closure.  In this
case, that meant <em>descriptor distances</em> had the right properties and
behaviors as binary coordinates were masked out. The final result led to
a method with decent loop-closure performance.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> Though others have
surpassed this, this technique had pretty strong performance at the
time. We are currently working on a method that incorporates additional
forms of insensitivity and a more efficient querying structure to arrive
at an efficient, online method with very high precision at 100% recall
for common benchmark loop closure data sets.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>S. Soatto. &ldquo;Steps Towards a Theory of Visual Information: Active
Perception, Signal-to-Symbol Conversion and the Interplay Between
Sensing and Control.&rdquo; 2011-2017.
<a href="https://arxiv.org/abs/1110.2053">arXiv</a>. He keeps changing the title
and the document. :-) <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>S. Soatto and A. Chiuso. &ldquo;Visual Representations: Defining Properties and Deep Approximations.&rdquo; 2014-2016. <a href="https://arxiv.org/abs/1411.7676">arXiv</a>. Also a living manuscript. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>S. Khan and D. Wollherr. &ldquo;IBuILD: Incremental Bag of Binary Words
for Appearance Based  Loop  Closure  Detection.&rdquo; <em>ICRA</em>, pp 5441-5447, 2015. <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>V. Balntas, L. Tang, and K. Mikolajczyk. &ldquo;BOLD - Binary Online
learned Descriptor  for  Efficient  Image  Matching. <em>CVPR</em>, pp 2367-2375, 2015. <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>G. Zhang, M.J. Lilly, and P.A. Vela. &ldquo;Learning Binary Features Online from Motion Dynamics for Incremental Loop-Closure Detection and Place Recognition.&rdquo; <em>ICRA</em>, pp. 765-772, 2016.  <a href="https://ieeexplore.ieee.org/document/7487205">Abstract</a>. IEEE <a href="https://ieeexplore.ieee.org/document/7487205">pdf</a>. <a href="https://arxiv.org/abs/1601.03821">arXiv</a>. <em>Best Robotic Vision Paper Award Finalist</em>. <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
         
      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="https://ivalab.github.io/RoboSLAM/public/js/jquery.min.js"></script>
<script src="https://ivalab.github.io/RoboSLAM/public/js/bootstrap.min.js"></script>
<script src="https://ivalab.github.io/RoboSLAM/public/js/jquery.cookie.js"> </script>
<script src="https://ivalab.github.io/RoboSLAM/public/js/ekko-lightbox.js"></script>
<script src="https://ivalab.github.io/RoboSLAM/public/js/jquery.scrollTo.min.js"></script>
<script src="https://ivalab.github.io/RoboSLAM/public/js/masonry.pkgd.min.js"></script>
<script src="https://ivalab.github.io/RoboSLAM/public/js/imagesloaded.pkgd.min.js"></script>
<script src="https://ivalab.github.io/RoboSLAM/public/js/owl.carousel.min.js"></script>
<script src="https://ivalab.github.io/RoboSLAM/public/js/front.js"></script>



</body>
</html>
